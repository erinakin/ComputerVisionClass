{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7478671d",
      "metadata": {
        "id": "7478671d"
      },
      "source": [
        "# ECES 681 - Computer Vision\n",
        "\n",
        "## Homework 3\n",
        "\n",
        "### Problem 1:\n",
        "\n",
        "\n",
        "\n",
        "Print out training losses and train and val set\n",
        "accuracy as it trains. After training concludes, also make a plot of the training losses as well as the training\n",
        "and validation-set accuracy of the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da12f30d",
      "metadata": {
        "id": "da12f30d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27de67b",
      "metadata": {
        "id": "d27de67b"
      },
      "source": [
        "### Load CIFAR-10 Dataset\n",
        "Load CIFAR-10 dataset for image classification. This dataset consists of 32 × 32 RGB images of 10 different categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8f1504",
      "metadata": {
        "id": "8f8f1504"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR10 dataset\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_set, val_set = random_split(train_dataset, [45000, 5000])\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f48c8d",
      "metadata": {
        "id": "70f48c8d"
      },
      "source": [
        "### Define Network Architecture\n",
        "This network has two FC layers with one ReLU activation layer: input -> FC layer -> ReLU layer -> FC\n",
        "layer -> scores. Write two_layer_net.ipynb function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60bd620f",
      "metadata": {
        "id": "60bd620f"
      },
      "outputs": [],
      "source": [
        "#  Define Network Architecture\n",
        "class TwoLayerNet(nn.Module):\n",
        "    \"\"\"A two-layer fully connected neural network.\"\"\"\n",
        "    def __init__(self, input_dim=3072, hidden_dim=100, num_classes=10):\n",
        "        \"\"\"Initialize the network with input dimension, hidden layer size,\n",
        "        and number of classes.\"\"\"\n",
        "        super().__init__()\n",
        "        # Fully connected layer 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # ReLU activated Layer\n",
        "        self.relu = nn.ReLU()\n",
        "        # Fully connected layer 2\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        # Flatten the tensor\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        # Apply ReLU activation\n",
        "        x = self.relu(x)\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f33cec",
      "metadata": {
        "id": "b9f33cec"
      },
      "outputs": [],
      "source": [
        "# Define a function to handle calculating the accuracy and loss of the model\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(data_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bbc0bb",
      "metadata": {
        "id": "64bbc0bb"
      },
      "source": [
        "### Train the Two-Layer Network with Loss and L2 Regularization\n",
        "Initialize the two-layer network and the optimizer using stochastic gradient descent, implement a training loop for training.\n",
        "\n",
        "\n",
        "The loss should be the sum of two terms:  \n",
        "• A data loss term, which is the softmax loss between the model’s predicted scores and the ground-truth\n",
        "image labels.  \n",
        "• A regularization loss term, which penalizes the L2 norm of the weight matrices of all the fully-connected\n",
        "layers of the model. You should not apply L2 regularization to the biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85261e3",
      "metadata": {
        "id": "b85261e3"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = TwoLayerNet(input_dim=32*32*3, hidden_dim=100, num_classes=10)\n",
        "\n",
        "learning_rate = 0.01\n",
        "# weight_decay parameter in torch automatically applies L2 regularization\n",
        "# to weights only\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Cross entropy loss deals with softmax loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690e2d93",
      "metadata": {
        "id": "690e2d93"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_acc = evaluate_accuracy(model, val_loader)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d09d4d9f",
      "metadata": {
        "id": "d09d4d9f"
      },
      "source": [
        "### Plot the Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b86a9a",
      "metadata": {
        "id": "95b86a9a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c623dc41",
      "metadata": {
        "id": "c623dc41"
      },
      "source": [
        "You will need to tune the hyperparameters of your model in order to improve it. Try changing the\n",
        "hyperparameters of the model. You can consider changing any of the following hyperparameters:  \n",
        "\n",
        "- num_train: The number of images to use for training\n",
        "- hidden_dim: The width of the hidden layer of the model\n",
        "- batch_size: The number of examples to use in each minibatch during SGD\n",
        "- num_epochs: How long to train. An epoch is a single pass through the training set.  \n",
        "- learning_rate: The learning rate to use for SGD  \n",
        "- reg: The strength of the L2 regularization term  \n",
        "\n",
        "You should tune the hyperparameters and train a model that achieves at least 40% on the validation\n",
        "set. In your homework submission, include the loss / accuracy plot for your best model. After tuning\n",
        "your model, run your best model exactly once on the test set.  \n",
        "Your model should not take an excessive amount of time to train. For reference, our hyperparameter settings\n",
        "achieve 42% accuracy on the validation set in less than 1 minute of training on a desktop with an Intel i9\n",
        "CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b5f926",
      "metadata": {
        "id": "f3b5f926"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}